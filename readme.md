---
output:
  pdf_document: default
  html_document: default
---


# REINFORCEMENT LEARNING AND COMPUTER VISION FOR AUTONOMOUS ROBOT CONTROL

This is a simple introduction to my Project about providing autonomous control to a robot using Deep Reinforcement Learning and Computer Vision

## Content of the project

In the implementation side we have **7 principle folders and files** in order of appearance:


- **Marker OBSTACLE_SHAPE.pdf** : Contains a another shape
- **Aruco Markers** : Contains the printable Aruco Markers used for the localization, and Sheets for obstacles.
  - **Aruco Marker ORIGINAL IDs_50_60 10_CM.pdf** : Are the References Markers, used for the definition of the mapped area, and for correcting the perspective. Used as one piece, when detected, it take the left-top position as reference for all the environment captured by the camera.
  - **Aruco Marker 6X6 250 ID_1 10_CM.pdf** : Contains the Position Marker for the target, when the reference is set, and the perspective is corrected, this Marker is used to track the target.
  - **Aruco Marker OBSTACLE_RECTANGLE.pdf** : Contains a rectangle shape with a *green* color that can be used as a real obstacle.
  -  **Marker OBSTACLE_SHAPE.pdf** : Contains a another shape with a *green* color that can be used as a real obstacle.
- **cam_01** : Contains the calibration values for the cameras, it's generated for my cameras, but because, we only need 2D position the calibration of my cameras, can work fine. Do not delete or change the path  of this folder to ensure code execution for **localization** and thus simulation.
- **agent.kv** : Contains the configuration of the widgets in the **Kivy** simulation. We will not touch it, but its important to unsure the execution of simulation.
- **ai_dqn.py** : Contains the implementation of the **DQN model** in the library **Pytorsh**, Provide the agent brain.
- **aruco_lib_localization_distributed_stand_alone.py** : Contains a simple implementation of the localization that can be run alone and give the core functionalities of the **Localization Module**.
- **aruco_lib_localization_distributed.py** : Contains the implementation of the **Localization Module** That can be used in other python code. This cannot run alone, it must be called and instantiated.
- **simulation_project.py** : The root file that utilize the localization module and the DQN module, and implement the simulation in Kivy. Run this file to execute the simulation, where the Agent train and act in the same time. Can be fully simulation, and we can introduce the localization and obstacles by using the Markers, and the obstacles sheets.
- other files are generated by the editor to ensure their function, and can be deleted if needed.




## How to use the code

### Install all the dependencies

if you have **python pip** isntalled run these in the command line:

```bash
> pip install numpy=1.18.1

> pip install pytorch=1.5.0

> pip install opencv-python==4.2.0.32

> pip install kivy==1.11.1

> pip install kivy-deps-glew==0.2.0

> pip install kivy-deps-sdl2==0.2.0

> pip install kivy-garden==0.1.4

> pip install guppy3==3.0.10

```

### Run the localization stand alone example



to run the exmaple from file **aruco_lib_localization_distributed_stand_alone.py**  but first follow those steps :

- Print the pdf files in **Aruco Markers** folders in provided sizes, or at least with the same aspect ratio. for the Markers, only compete balck color. For the obstacles print in color with the  true color in the sheets.
- Provide and connect a **Web Camera**, with prefered resolution of **480p** to repriduce the same conditions I have. alghout the perspective is fixed, so technically the major functionalities must be the same for all cases. Except some Frames.
- Go to line **510** and set the camera id, in the most cases, if the camera is the only camera attached by the usb cable the id equal **1**, but, if it doesn't work check the log and see the message : **Camera Status**. to change the camera id : see the line **510**

```python
#--- camera id
camera_id = 1  #set id here

```
- Run code from the command line:
```bash
> python aruco_lib_localization_distributed_stand_alone.py
```
I recommand install **Microsofr VS Code with the python extention** to have smoth and easy runung of the code. 
Anyway these are the availables steps:

* If the code run successfully you notice some 5 windows appeared. Each one contains a different view or information, the imoprant ones are :

  + **Frame Original with infos** : Contains the orginal stream plus the informatio about the presence of  **reference Markers**
  + **Perspective with position** : Contains the corrected perspective and the mapped irea. And the imformation about the presence of the **target Marker**, in the case of no reference found, the frame withh be the same as the orignial stream but can detect position of the **target Marker** with indection of no perspective detected.
  + **Perspective with obstacles** : Contains the detcted obstacles in the obstacles sheets. Still work even in no presence of reference, but it return a value indicating that.
* Try using to put the camera in a high position like in the report figures and try with the markers and obstacles. to see the functining.
* To quit press **q** and all threads will stop, and winodows will go immedialtally.


### Run the simulation of all functinalities

In this part we will explain the execution of the simulation of the **Agent** that represent the autonomous robot. the script to run is **simulation_project.py**, like as I said with the localization exaple you can run it in **VS Code** and if not avoilable open folder **FINAL CODE** in command line and  type :

```bash
> python simulation_project.py

```
But before that follow these steps, the first 2 steps are the same as in the execution of localization example, anyway :

* Print the pdf files in **Aruco Markers** folders in provided sizes, or at least with the same aspect ratio. for the Markers, only compete balck color. For the obstacles print in color with the  true color in the sheets.
* Provide and connect a **Web Camera**, with prefered resolution of **480p** to repriduce the same conditions I have. alghout the perspective is fixed, so technically the major functionalities must be the same for all cases. Except some Frames.
- Go to line **1** and change the camera id:

```python 
camera_id = 1 #set id camera here

```
* If there is any error in the camera check the logs for **Camera Status** 
* If Everything run corrctally you should see the Kivy Window with title **DRL and CV for Autonomous Robot** and you can see more wondows for the localization, all the winodws and how to get them:

  + **DRL and CV for Autonomous Robot** : The **principal window**, you get it when you run the script, it contains the main canvas where the movement of the **Agent** is semulated by a white rectange, and having three dot, representing the sensors. The agent is in aconstant move folloying another white disque/point that represent the **Target**. There is three buttons, we will explain them also.
  + **Frame Original with infos** , **Frame Original** : Touch the principal window and Press **r** to get these windows, and press again **r** while you are on principal window to close them if you want. These frames contains the original stream and information abou the reference Markers.
  + **Perspective with position**, **Perspective** :   + **Frame Original with infos** , **Frame Original** : Touch the principal window and Press **p** to open them, and if you want to close them, click on principal window and press again **p**. These windows provide the maped irea with the correct perspective, and also information about the postion of the target Marker.
  + **Perspective with obstacles** : This window contains the obstacles if the mapped irea is provided, to open it be on the principal window and press **o**, and to close it press also **o**, while you are on the principale window.
  
* On the principale window, to close all other windows press **q**.

* On the principale window, you can play with the agent, you can add obstacles, using the mouse, or you can add real obstacles and target usong the printed Markers, and Obstacles sheets.
* On the principale window, there is three buttons:

  + **Clear** : To delete the simulated obstacle only from the simulation canvas and the obstacles matrix.
  + **Save** : To save the **Neural Network** or the brain of the agent, and it will show the prerformence pf the agent, by calculating the mean of the reward in the last 1000 interactions.
  + **Load** : To use an already trained NN from previous sessions.

* **To quit correctally** the simulation press **esc**/escape key. This will stop thread and finish the simulation.

* Try to expimiment with the use of obstacles and markers and see the performance each time.

I hope this snmall guide is usefull to use this pieces of codes.\ 

This is all, for any more help please contact me at : iammohamedhassani[at]gmail.com
  
